<?xml version="1.0" encoding="UTF-8"?>
<!-- This Source Code Form is subject to the terms of the Mozilla Public
   - License, v. 2.0. If a copy of the MPL was not distributed with this
   - file, You can obtain one at http://mozilla.org/MPL/2.0/. -->
<!DOCTYPE html>
<?xml-stylesheet href="chrome://global/skin/" type="text/css"?>
<html id="feedHandler" xmlns="http://www.w3.org/1999/xhtml">
  <head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <meta http-equiv="Content-Security-Policy" content="default-src chrome:; img-src *; media-src *" />
    <title>Phillip Mates' Articles</title>
    <link rel="stylesheet" href="chrome://browser/skin/feeds/subscribe.css" type="text/css" media="all" />
  </head>
  <body>
    <div id="feedHeaderContainer">
      <div id="feedHeader" dir="ltr" class="feedBackground" firstrun="true">
        <div id="feedIntroText">
          <p id="feedSubscriptionInfo1">This is a “feed” of frequently changing content on this site.</p>
          <p id="feedSubscriptionInfo2">You can subscribe to this feed to receive updates when this content changes.</p>
        </div>
        <div id="feedSubscribeLine">
          <label id="subscribeUsingDescription">Subscribe to this feed using
            <select id="handlersMenuList">
              <option id="liveBookmarksMenuItem" selected="selected">Live Bookmarks</option>
              <option disabled="true">━━━━━━━</option>
            <option id="selectedAppMenuItem" handlerType="client" style="display: none;"></option><option id="defaultHandlerMenuItem" handlerType="client" style="display: none;"></option><option id="chooseApplicationMenuItem">Choose Application…</option></select>
          </label>
          <label id="checkboxText">
            <input type="checkbox" id="alwaysUse" class="alwaysUse" />Always use Live Bookmarks to subscribe to feeds.</label>
          <button id="subscribeButton">Subscribe Now</button>
        </div>
      </div>
      <div id="feedHeaderContainerSpacer"></div>
    </div>
    <div id="feedBody">
      <div id="feedTitle">
        <a id="feedTitleLink">
          <img id="feedTitleImage" />
        </a>
        <div id="feedTitleContainer">
          <h1 id="feedTitleText" xml:base="http://www.ccs.neu.edu/home/mates/articles.atom">Phillip Mates' Articles</h1>
          <h2 id="feedSubtitleText"></h2>
        </div>
      </div>
      <div id="feedContent"><div class="entry"><h3><a href="http://ccs.neu.edu/home/mates/articles/contract-verification.html"><span xml:base="http://www.ccs.neu.edu/home/mates/articles.atom">Contract Verification using Symbolic Execution</span></a><div class="lastUpdated">February 19, 2013, 1:00 AM</div></h3><div xml:base="http://www.ccs.neu.edu/home/mates/articles.atom" class="feedEntryContent"><p>For <a href="http://www.ccs.neu.edu/home/shivers/">Olin Shiver</a>'s Program Analysis course this semester, I'm presenting a lecture on “<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">Higher-Order Symbolic Execution via Contracts</a>” by <a href="http://www.ccs.neu.edu/home/samth/">Sam Tobin-Hochstadt</a> and <a href="http://www.ccs.neu.edu/home/dvanhorn/">David Van Horn</a>.
 I'm writing this post to prepare for my presentation, hence it is 
mostly a reinterpretation of the paper mentioned. With that said, let's 
get started!</p>
<p>Adapting symbolic execution to work with contracts provides two main 
gains: increasing the efficiency of running programs with contract and 
allowing for modular program analysis.</p>
<p>Programming with contracts enables programmers to provide rich 
dynamic specifications to components. With the ease of writing 
specifications dynamically comes the expense of checking them at 
runtime. The ability to statically prove the absence of contract 
violations would provide a large efficiency gain, enabling contract 
checks excluded from the execution.</p>
<p>Contracts also enable us to reason about modules in isolation, or 
modularly, since they specify how two parties should interact at their 
boundaries. As it turns out, analyzing modular programs is difficult! 
Abstract interpretation for instance, cannot reason about concrete 
function flowing into unknown contexts. Symbolic execution allows for 
analysis in the presence of unknown components but often fails to offer 
automatic, terminating analyses. If we can utilize abstract 
interpretation and symbolic execution to verify contracts, we'll be left
 with a modular and composable automatic program analysis.</p>
<p>With these two problems in mind, let's delve into the intricacies of the paper. On a high level, [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>]
 extends CPCF, a language with contracts, to operate 
(non-deterministically) over symbolic values (SCPCF) in order to soundly
 discover blame. This is done by refining symbolic values with 
successful contract checks and using those to skip future contract 
checks.</p>
<p>We'll start with an overview of contracts, move to an overview of 
symbolic execution, and close with how the semantics of contract PCF is 
extended to include symbolic values.</p>
<h2 id="contracts">1. Introduction to Contracts</h2>
<p>Contracts are executable specifications that enable blame to be 
assigned to software components that violate defined specifications. At 
the first-order level, blame assignment and checking of contracts is 
simple, but higher-order contracts are limited by Rice's Theorem: it 
isn't decidable whether a function argument adheres to its contract. 
Hence higher-order values must be wrapped in a monitor that checks the 
contract at each use. Monitors track agreements between parties as 
higher-order values flow back and forth across component boundaries, 
enabling blame to be assigned correctly.</p>
<p>Summarizing from [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>], CPCF is an extension of PCF, introduced by Dimoulas et al. [<a href="http://www.ccs.neu.edu/racket/pubs/esop12-dthf.pdf">7</a>], that enables contracts to wrap base values and first class functions.</p>
<pre><code>Types       T ::= B | T → T | con(T)
Base types  B ::= N | B
Terms       E ::= A | X | E E | μ X:T . E | if E E E
                | O1(E) | O2(E, E) | mon f f f (C, E)
Operations O1 ::= zero? | false? | ...
           O2 ::= + | - | ∧ | ∨ | ...
Contracts   C ::= flat(E) | C → C | C → λ X:T . C
Answers     A ::= V | ε[blame f f]
Values      V ::= λ X:T . E | 0 | 1 | -1 | ... | tt | ff
Evaluation  ε ::= [] | ε E | V ε | O2(ε, E) | O2(V, ε)
  contexts      | O1(ε) | if ε E E | mon f,g h(C, ε)</code></pre>
<p>There are three types of contracts: <code>flat(E)</code> wraps base values with a predicate defined in the CPCF language, <code>C1 → C2</code> is a function contract with the contract <code>C1</code> for the function's argument, and <code>C2</code> for the function's result, lastly <code>C1 → λX.C2</code> are dependent contracts where <code>X</code> binds the function's argument in the body of <code>C2</code>.</p>
<p>A contract <code>C</code> is attached to expression <code>E</code> using the monitor construct <code>mon f,g h (C, E)</code>, where the labels <code>f</code>, <code>g</code> and <code>h</code> are used to blame specific components in the event of a contract failure. Label <code>f</code> names the server or the actual expression, <code>g</code> labels the client, or the context, and <code>h</code> names a contract, since contracts are arbitrary expressions, they can also be blamed. The labels in <code>blame f g</code> state that component <code>f</code> broke its contract with <code>g</code>.</p>
<p>[<a href="http://www.ccs.neu.edu/racket/pubs/esop12-dthf.pdf">7</a>] 
introduces complete monitors, which correctly monitor all channels of 
communication between components, including contracts themselves. For 
the sake of simplicity, we will work with lax contract semantics, which 
is not a complete monitor, but is still blame correct.</p>
<p>The reduction relation of CPCF is mostly standard, with the last two rules handling function and flat contract checks.</p>
<pre><code>if tt E1 E2                   ⟼ E1
if ff E1 E2                   ⟼ E2
(λ X:T . E) V                 ⟼ E[V/X]
μ X:T . E                     ⟼ E[μ X:T . E/X]
O(V)                          ⟼ A if δ(O, V) = A
mon f,g h(C1 → λ X:T . C2, V) ⟼ λ X:T . mon f,g h (C2, (V mon g,f h (C1, X)))
mon f,g h(flat(E), V)         ⟼ if (E V) V blame f h</code></pre>
<p>Function contracts are checked by reversing the monitor's blame 
labels on the argument, since if it doesn't satisfy the domain of the 
function contract, it is the context's fault. The monitor of the 
computation's result retains its original blame labels since a result 
that doesn't satisfy the range of the contract is the function's fault. 
Flat contract checking involves checking the value against the contract 
in an <code>if</code> expression, raising blame if the contract isn't satisfied.</p>
<p>Here is a simple example adapted from [<a href="http://www.ccs.neu.edu/racket/pubs/icfp2002-ff.pdf">9</a>] that shows why label reversal happens for higher-order contracts:</p>
<pre class="sourceCode Scheme"><code class="sourceCode scheme"><span class="co">;; foo : (Int → Int) → Int</span>
<span class="co">;; with contract: ((greater-than-9? → between-0-and-10?) → between-10-and-20?)</span>
(<span class="kw">define</span><span class="fu"> foo </span>(λ (bar) (<span class="kw">+</span> <span class="dv">10</span> (bar <span class="dv">0</span>))))</code></pre>
<p>When <code>foo</code> invokes <code>bar</code>, <code>foo</code>'s <code>greater-than-9?</code> contract fails, but this is due to <code>foo</code> supplying the incorrect value to <code>bar</code>. The <code>greater-than-9?</code> contract is to the left of two arrows in <code>foo</code>'s
 contract, and is said to be in an even position. Base contracts in even
 position have had their blame labels reversed by the reduction relation
 an even number of times, so blame will always go to the function.</p>
<p>On the other hand, if we imagine that <code>foo</code> applies <code>bar</code> to <code>10</code> and it returns <code>-10</code>, then this will violate <code>between-0-and-10?</code>,
 which is to the left of one arrow, and thus in an odd position. Being 
in an odd position means that the reduction relation has reversed the 
blame labels an odd number of times, leaving blame to be assigned to the
 context that provided the <code>bar</code> argument.</p>
<p>From here [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>]
 introduces SCPCF, an extension of CPCF that include symbolic values. 
Before we get into that, I'd like to cover the basics of symbolic 
execution.</p>
<h2 id="symb">2. Introduction to Symbolic Execution</h2>
<p>Symbolic execution is a form of program analysis that lies between 
the extremes of testing a program and proving it correct. The idea is to
 generalize the notion of testing a program on concrete inputs by 
replacing them with symbolic input variables. These symbolic variables 
represent a broad class of concrete inputs. Hence, the result of a 
program's symbolic execution is equivalent to a large number of normal 
test cases run over that program.</p>
<p>To run a program symbolically, the execution semantics of the 
language must be extended to let operators work on symbolic inputs, 
producing symbolic formulas as output. Symbolic inputs are arbitrary 
symbols, and symbolic formulas are manipulations over these symbols, 
suited to capture data-specific information. If we are trying to 
symbolically executing arithmetic operations, the formulas outputted by 
symbolic operations will be algebraic formulas, enabling us to leverage 
algebraic simplifications. For instance, extending the concrete version 
of <code>+</code>, <code>δ(+, 4, 5) = 9</code>, to operate over symbolic values returns an algebraic equation: <code>δ(+, α1, α2) = α1 + α2</code>.</p>
<p>Let's look at the following simple arithmetic function:</p>
<pre class="sourceCode Scheme"><code class="sourceCode scheme"><span class="co">;; doubleFirst :: Int -&gt; Int -&gt; Int</span>
(<span class="kw">define</span><span class="fu"> </span>(doubleFirst x y)
  (<span class="kw">let</span> ([z (<span class="kw">+</span> x y)])
    (<span class="kw">+</span> x (<span class="kw">-</span> z y))))</code></pre>
<p>Executing the it with concrete values, <code>(doubleFirst 4 5)</code>, leaves us with the following variable assignments: <code>x = 4, y = 5, z = 9, return = 8</code>. With the symbolic execution <code>(doubleFirst α1 α2)</code>, the function's variables are assigned to the following algebraic formulas <code>x = α1, y = α2, z = α1 + α2, return = α1 + α1</code>.</p>
<p>So far, the symbolic execution we've described can only handle 
programs with linear control flow. How do we handle branching operations
 in the presence of symbolic values? Symbolic values generally won't 
have enough constraints to determine which branch of a branching 
operation to take, hence we need to add the <em>path conditional</em> (<code>pc</code>)
 to our program's execution state. The path conditional is a boolean 
expression over symbolic values that accumulates constraints which must 
be satisfied in order for the execution to follow the associated path.</p>
<p>Let's run through a symbolic execution of the <code>divCond</code> function below instantiated with symbolic values <code>α1</code> and <code>α2</code> and starting with <code>pc = true</code>.</p>
<pre class="sourceCode Scheme"><code class="sourceCode scheme"><span class="co">;; divCond :: Int -&gt; Int -&gt; Int</span>
(<span class="kw">define</span><span class="fu"> </span>(divCond x y)
  (<span class="kw">if</span> (== y <span class="dv">10</span>)
    (<span class="kw">if</span> (== x <span class="dv">0</span>)
      (error <span class="st">"div by zero"</span>)
      (<span class="kw">/</span> y x))
    x))</code></pre>
<p>At the <code>(== y 10)</code> expression, <code>y = α2</code>, so we don’t have enough information to branch on it, hence we must fork:</p>
<ul>
<li>For the “then” case we set <code>pc = true ∧ (α2 == 10)</code> and process the <code>(if (== x 0) ...)</code> expression, where:
<ul>
<li>in the “then” case <code>pc = true ∧ (α2 == 10) ∧ (α1 == 0)</code> and an error is raised</li>
<li>in the “else” case <code>pc = true ∧ (α2 == 10) ∧ (α1 != 0)</code> and a division is computed</li>
</ul></li>
<li>the “else” case <code>pc = true ∧ (α2 != 10)</code> and we return <code>x</code>, which is set to <code>α1</code></li>
</ul>
<p>At this point, you might be wondering how such obvious properties could be useful?</p>
<p>Godefroid, Klarlund and Sen [<a href="https://research.microsoft.com/en-us/um/people/pg/public_psfiles/pldi2005.pdf">4</a>]
 use randomized program testing paired with symbolic execution to 
efficiently generate concrete test inputs with complete code coverage. 
The idea is to run the program on randomized input while keeping track 
of the path conditional. This execution will cover one path through the 
program, but they are seeking complete path coverage, so they backtrack 
to the last place where a constraint was added to the <code>pc</code>, negate that constraint and generate new test input that satisfies the new <code>pc</code>.
 When the program is run on this new test input, it will take go down 
the other branch of the last conditional, covering an additional path. 
Repeating this process and generating concrete test inputs for each path
 through a program results in a minimal set of path covering test 
inputs.</p>
<p>Cousot, Cousot, Fähndrich and Logozzo [<a href="http://research.microsoft.com/pubs/174239/paper.pdf">5</a>] use abstract interpretation, a technique similar to symbolic execution, to infer <em>necessary preconditions</em>
 to functions in a 1st order setting. By necessary precondition, we mean
 the smallest possible precondition, which when violated will definitely
 result in an error. The general idea behind their approach, which can 
be described in terms of symbolic execution, is to identify the set of 
path conditions that lead to errors, negate them and set them as 
preconditions to the function. Take for example the symbolic execution 
of <code>divCond</code>, the only error occurs when the <code>pc</code> is <code>(α2 == 10) ∧ (α1 == 0)</code>, thus the necessary precondition for <code>divCond</code> is <code>¬((y == 10) ∧ (x == 0))</code>.</p>
<p>In general, symbolic execution is used widely in model checking and 
test case generation. Now let's move on to how it can be leveraged to 
verify contracts.</p>
<h2 id="contract-symb">3. Contracts as symbolic values</h2>
<p>We are looking to be able to verify that the composition of known and
 unknown program components respects their specifications. To accomplish
 this, [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>] introduces the notion of symbolic values to CPCF, creating SCPCF. These symbolic values, denoted by <code>●T</code>, represent unknown values of type <code>T</code>. They have arbitrary behavior of type <code>T</code>,
 which is then refined “by attaching a set of contracts that specify an 
agreement between the program and the missing component”.</p>
<pre><code>Prevalues      U ::= ●T | λ X:T . E | 0 | 1 | -1 | ... | tt | ff
Values         V ::= U/Ç    where Ç = {C,...}</code></pre>
<p>Prevalues are the normal values of CPFC extended with the symbolic <code>●T</code>. Values are prevalues extended with a set of contracts. Notationally, <code>V ∙ C</code> will be used to represent <code>U/Ç ∪ {C}</code> where <code>V = U/Ç</code>, and <code>Ç</code> will be omitted from values wherever irrelevant.</p>
<p>The goal is to define the semantics of SCPCF to run with unknown 
values such that it soundly approximates running the same program with 
concrete values. More formally, the semantics are sound, or preserve 
approximation, if given <code>⊢ V:T ⊑ ●T</code> and <code>ε[V] ⟼* A</code>, then <code>ε[●T] ⟼* A'</code> and <code>A ⊑ A'</code>, that is, <code>A'</code> approximates <code>A</code>.
 Hence, we can run a program with unknown values, and if it doesn't 
raise blame, then we know that a concrete instantiation of that program 
will also not raise blame.</p>
<h2 id="scpcf-semantics">4. SCPCF Semantics</h2>
<p>We extend the semantics much like described in <a href="#symb">§2</a>, but with some key differences:</p>
<pre><code>if V E1 E2      ⟼ E1   if δ(false?, V) ∋ ff
if V E1 E2      ⟼ E2   if δ(false?, V) ∋ tt
(λ X:T . E) V   ⟼ E[V/X]
μX:T.E          ⟼ E[μX:T . E/X]
O(V)            ⟼ A if δ(O, V) ∋ A
(●T→T'/Ç) V     ⟼ ●T'/{C2[V/X] | C1 → λ X:T . C2 ∈ Ç}
(●T→T'/Ç) V     ⟼ havocT V</code></pre>
<p>Before <code>δ</code> mapped arithmetic operations to algebraic equations, which satisfy multiple concrete answers. We let the <code>δ</code>
 of SCPCF map operations and arguments to sets of answers. Concrete 
arguments map to concrete singleton sets while symbolic arguments map to
 more abstract sets, for instance <code>δ(+, V1, V2) = {●N}</code>, if <code>V1</code> or <code>V2 = ●N/Ç</code></p>
<p>Due to the change of <code>δ</code>, branching becomes non-deterministic in the presence of symbolic values, similar to <a href="#symb">§2</a>.
 Unlike before though, we don't add information to the path condition at
 branches. This is because we aren't really concerned about gathering 
constraints on symbols at the term level, but rather at the contract 
level.</p>
<p>That said, since contracts use the same language as terms, the theory in [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>]
 could be made more precise if branches hoisted their predicates into 
contracts and added them to the path condition upon non-deterministic 
forks. For instance, given <code>foo = ●N</code> and that <code>sqrt</code> has the contract <code>flat(positive?) → flat(positive?)</code>, then <code>(if (positive? foo) (sqrt foo) 0)</code> cannot be verified using [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>] unless evaluating <code>(positive? foo)</code> resulted in <code>foo ∙ flat(positive?)</code></p>
<p>The last two rules of the SCPCF semantics deal with applying symbolic
 functions. There are two possible outcomes when a argument <code>V</code> is passed to an unknown function <code>●T→T'</code>:</p>
<ul>
<li><code>V</code> is used in an unknown manner, but when <code>V</code> is used, it returns with no failures. Hence the result of the use of <code>V</code> is refined by the range of the contracts over the function <code>V</code>.</li>
<li>Alternatively, the usage of <code>V</code> in an unknown context results in the blame of <code>V</code>. Possible blame is found using the <code>havoc</code> function, which explores the behavior of <code>V</code> by iteratively applying it to unknown values.</li>
</ul>
<pre><code>havocB    = μx.x
havocT→T' = λx:T → T' . havocT'(x ●T)</code></pre>
<p>Values of base type can't raise blame in an unknown context, since 
their contract has already been satisfied when they were passed into the
 unknown context. Thus, <code>havocB</code> diverges as a means to not introduce spurious results. At the function type, <code>havoc</code>
 “produces a function that applies its argument to an appropriately 
typed unknown input value and then recursively applies havoc at the 
result type to the output”. This soundly explores possible blame 
behavior of values in unknown “demonic” contexts.</p>
<p>Let's slowly work through the example presented in [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>] to see how <code>havoc</code> will discover blame, if it exists:</p>
<p>Let <code>hoExample</code> be a higher-order function with the contract <code>(any→any)→any</code> and let <code>sqrt</code> have the contract <code>positive? → positive?</code>.</p>
<pre><code>hoExample:(N→N)→N =
  λ f:N→N . mon(any,
                (λ f:N→N . (sqrt (f 0))) (λ x:N mon(any, f mon(any, x))))</code></pre>
<p>We want to check that in any arbitrary context, <code>hoExample</code> cannot be blamed, hence we pass it to <code>havoc((N→N)→N)</code>.</p>
<pre><code>1.  havoc(N→N)→N hoExample
2.  havocN (hoExample ●N→N)
3.         mon(any, (λ f:N→N . (sqrt (f 0))) (λ x:N mon(any, ●N→N mon(any, x))))
4.                  (sqrt ((λ x:N mon(any, ●N→N mon(any, x))) 0))
5.                  (sqrt mon(any, ●N→N mon(any, 0)))
6.                  (sqrt mon(any, (●N→N 0)))
7a.  ...     ...    (sqrt havocN 0)
7b.                 (sqrt ●N∙any)
8.                  ((λ x:N . mon(positive?, sqrt mon(positive? x))) ●N∙any)
9.                  mon(positive?, sqrt mon(positive?, ●N∙any))
10. havocN mon(any, blame hoExample)</code></pre>
<p>In step 2, <code>havoc</code> applies <code>●N→N</code> to <code>hoExample</code>, using <code>havocN</code> to ensure divergence if no blame is found. Step 3 and 4 substitute <code>●N→N</code> into <code>hoExample</code>, wrapping it in a <code>any→any</code> monitor. <code>0</code> passes the <code>any</code> contract in steps 5 and 6. In step 7, the evaluation of the symbolic function <code>●N→N</code> applied to <code>0</code> introduces a fork in the reduction relation: on one branch <code>havocN</code> is applied to <code>0</code> to check if it introduces blame, and on the other, <code>(●N→N 0)</code> evaluates with no blame, resulting in <code>●N∙any</code>. <code>havocN 0</code> diverges, so we continue with the second branch. In step 8 and 9 <code>●N∙any</code> is passed to <code>sqrt</code>, but the contract check <code>(positive? ●N∙any)</code> both passes and raises blame, so validation of <code>hoExample</code> fails.</p>
<p>If we change <code>hoExample</code>’s contract to <code>(any→positive?)→any</code>, then in step 7b the symbolic value <code>●N</code> would be refined with the contract <code>positive?</code>, enabling the contract check of step 9 to pass. This is made possible with the introduction of the <code>⊢ V : C ✓</code> relation presented below.</p>
<h2 id="checking-scpcf">5. Contract checking in SCPCF</h2>
<p>Adding non-deterministic branching at <code>if</code> statements in 
the presence of symbolic values means that the semantics of flat 
contract checking becomes quite imprecise, if left as is. Using the 
contract checking semantics of <a href="#contracts">§1</a> with symbolic values means that the following expression will incorrectly raise blame twice:</p>
<p><code>(mon f,g h (flat(prime?) → flat(even?), primePlus1)) (mon f,g h (flat(prime?) ●N))</code></p>
<p>Once when monitoring <code>●N</code> to see if it satisfies <code>flat(prime?)</code>, which results in both success and failure since <code>(prime? ●N) = {tt, ff}</code>, and the other when <code>●N</code> is checked to satisfy the domain of <code>flat(prime?) → flat(even?)</code>, as it’s passed to <code>primePlus1</code>.</p>
<p>To remedy this, we set up the path condition to keep track of when 
symbolic values satisfy a given contract. With this information, future 
contract checks can be ruled out, thus eliminating non-deterministic 
branching and unsound blame.</p>
<p>The path condition in [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>] is formulated by refining symbolic values with contracts (<code>V∙C</code>) once a flat contract check passes. This information is then used in the following judgement relation, which says that <code>V</code> provably satisfies the contract <code>C</code>:</p>
<pre><code>   C ∈ Ç
───────────
 ⊢ V : C ✓</code></pre>
<p>With these additions we can modify the contract checking semantics, 
remembering successful contract checks and avoiding the imprecision 
introduced by redundant contract checks:</p>
<pre><code>mon f,g h (C,V)       ⟼ V if ⊢ V : C ✓
mon f,g h (flat(E),V) ⟼ if (E V)
                          (V ∙ flat(E))    | where ⊬ V : flat(E) ✓
                          blame f g        |
mon f,g h (C1 ⟼ λ X:T . C2, V) ⟼ λ X:T . mon f,g h (C2, V mon g,f h (C1, X))
                                   where ⊬ V : C1 ⟼ λ X:T . C2 ✓</code></pre>
<h2 id="soundness-scpcf">6. Soundness of SCPCF</h2>
<p>As noted in <a href="#cont">§3</a>, we are striving for a symbolic 
execution semantics that soundly approximates the concrete execution 
semantics. By sound approximation we mean that if the symbolic semantics
 doesn't raise blame during an execution, then the concrete semantics is
 guaranteed to also not raise blame. Of course it is an approximation, 
so it is acceptable for the symbolic semantics to find blame when it 
doesn't actually exists.</p>
<p>In order to prove that the symbolic execution semantics soundly 
approximates the concrete execution semantics when concerned with blame,
 we develop an approximation relation <code>⊑</code> where <code>A ⊑ A'</code> means that <code>A'</code> approximates, or is less precise than <code>A</code>:</p>
<pre><code>                  ⊢ V : T
                 ──────────        ───────────────────
                  ⊢ V ⊑ ●T          mon(C, E) ⊑ ● ∙ C

                          ⊢ V : C ✓             mon(C, E) ⊑ E'
     ───────────         ───────────       ────────────────────────
      V∙C ⊑ V              V ⊑ V∙C          mon(C, E) ⊑ mon(C, E')

           ──────────────────────────────────────────────────
            (λ X . mon(D, (V mon(C, X)))) ⊑ ● ∙ C → λ X . D</code></pre>
<p>From top to bottom, left to right, the rules are as follows:</p>
<ul>
<li>unknown values of type T approximate concrete values of type T</li>
<li>unknown values refined by a contract approximate expressions monitored with the same contract</li>
<li>contract refinements may be introduced on the value being approximated</li>
<li>refinements may be eliminated from the approximating value when that value already proves the contract</li>
<li>a contract monitor may be introduced to the approximation expression
 if the expression already approximates a value wrapped in the same 
contract monitor</li>
<li>unknown values refined with function contracts approximate a value monitored by a partially evaluated function contract</li>
</ul>
<p>The soundness theorem:</p>
<pre><code>E ⊑ E' ∧ E ⟼* A implies ∃ A' . E' ⟼* A' ∧ A ⊑ A'</code></pre>
<p>is proved by case analysis on the reduction semantics and utilizing 
havoc's completeness, as well as auxiliary lemmas that ensure 
substitution and basic operations preserve approximation. By havoc's 
completeness, we mean <code>ε[V] ⟼* ε'[blame l]</code> where <code>l</code> is not in <code>ε</code>, then <code>havoc V ⟼* ε''[blame l]</code>.</p>
<p>As a small aside, last semester I used Coq to prove that a 
non-deterministic semantics for a simple, typed language with unknown 
values (<code>●T</code>) soundly approximates its concrete counterpart (<a href="https://github.com/phillipm/black-hole-analysis">coq code on github</a>).</p>
<p>And that is pretty much it! The symbolic semantics are not guaranteed to terminate, so the authors of [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>] integrate the orthogonal technique of abstracting abstract machines [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/vanhorn-might-cacm11.pdf">8</a>]. Additionally, [<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">1</a>]
 goes on to extend their technique to an untyped core model of Racket, 
which makes things more complicate in the absence of type information.</p>
<h2 id="conc">8. Conclusion</h2>
<p>In this post I've presented the fundamental ideas from “<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">Higher-Order Symbolic Execution via Contracts</a>” by <a href="http://www.ccs.neu.edu/home/samth/">Tobin-Hochstadt</a> and <a href="http://www.ccs.neu.edu/home/dvanhorn/">Van Horn</a>.</p>
<p>This paper introduced the idea of a modular analysis for programs 
with higher-order contracts that enables contract verification in the 
presence of unknown components. Such a verification enables the safe 
omission of select contract checks and also provides an analysis where 
individual components can be verified for later composition. Symbolic 
execution with contracts as symbol refinements is the secret weapon, 
with other key components being demonic contexts (<code>havoc</code>) to discover blame, the approximation relation (<code>⊑</code>) to prove soundness, and abstract interpretation to guarantee termination.</p>
<p>Possible future work:</p>
<ul>
<li>Adapting the work on necessary precondition inference by [<a href="http://research.microsoft.com/pubs/174239/paper.pdf">5</a>]
 to work in a higher-order setting. This could be used to strengthen the
 contracts of known components so that their composition with unknown 
components can be successfully verified.</li>
<li>Hoisting arbitrary predicate checks to the contract level and adding
 them to the path condition, or contract set of a value, as described in
 <a href="#scpcf-semantics">§4</a>.</li>
</ul>
<p>Questions:</p>
<ul>
<li>How does symbolic execution traditionally deal with non-termination?</li>
<li>Can havoc be made more precise by integrating path covering test case generation similar to [<a href="https://research.microsoft.com/en-us/um/people/pg/public_psfiles/pldi2005.pdf">4</a>]?</li>
<li>Would weakening of contracts ever increase the ability of the analysis to verify a component's contracts?</li>
</ul>
<h2 id="links">Links</h2>
<ul>
<li>[1] “<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/tobin-hochstadt-vanhorn-oopsla12.pdf">Higher-Order Symbolic Execution via Contracts</a>” by Tobin-Hochstadt and Van Horn 2012</li>
<li>[2] “<a href="http://www.cs.uiuc.edu/class/sp11/cs477/king76symbolicexecution.pdf">Symbolic Execution and Program Testing</a>” by King 1976</li>
<li>[3] “<a href="http://users.eecs.northwestern.edu/%7Erobby/pubs/papers/fb-tr2006-01.pdf">Contracts as Pairs of Projections</a>” by Findler and Blume 2006</li>
<li>[4] “<a href="https://research.microsoft.com/en-us/um/people/pg/public_psfiles/pldi2005.pdf">DART: Directed Automated Random Testing</a>” by Godefroid, Klarlund and Sen 2005</li>
<li>[5] “<a href="http://research.microsoft.com/pubs/174239/paper.pdf">The Design and Implementation of a System for the Automatic Inference of Necessary Preconditions</a>” by Cousot, Cousot, Fähndrich and Logozzo 2013</li>
<li>[6] “<a href="http://www.cs.purdue.edu/homes/zhu103/pubs/vmcai13.pdf">Compositional and Lightweight Dependent Type Inference for ML</a>” by Zhu and Jagannathan 2013</li>
<li>[7] “<a href="http://www.ccs.neu.edu/racket/pubs/esop12-dthf.pdf">Complete Monitors for Behavioral Contracts</a>” by Dimoulas, Tobin-Hochstadt, and Felleisen 2012</li>
<li>[8] “<a href="http://www.ccs.neu.edu/home/dvanhorn/pubs/vanhorn-might-cacm11.pdf">Abstracting Abstract Machines</a>” by Might and Van Horn 2011</li>
<li>[9] “<a href="http://www.ccs.neu.edu/racket/pubs/icfp2002-ff.pdf">Contracst for Higher-Order Functions</a>” by Findler and Felleisen 2002</li>
</ul></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://ccs.neu.edu/home/mates/articles/compiler-verification-intro.html"><span xml:base="http://www.ccs.neu.edu/home/mates/articles.atom">An Introduction to Compiler Verification</span></a><div class="lastUpdated">January 28, 2013, 1:00 AM</div></h3><div xml:base="http://www.ccs.neu.edu/home/mates/articles.atom" class="feedEntryContent"><p>Last semester I started working on a compiler verification project with <a href="http://www.ccs.neu.edu/home/amal/">Amal Ahmed</a>. We've been investigating whether the CPS transformation presented in [<a href="http://www.ccs.neu.edu/home/amal/papers/epc.pdf">1</a>]
 continues to preserve equivalence when the source and target languages 
are extended with recursive types, and hence, non-termination. On a high
 level, it looks like the equivalence preservation property should hold,
 yet after some initial progress we've become stuck on proving a 
continuation shuffling lemma. I plan to write more on that soon, but for
 now I'd like to give an introduction to compiler verification.</p>
<h2 id="what-are-we-verifying-about-compilers">What are we verifying about compilers?</h2>
<p>Compilers are software that transform programs from a source language
 into programs of a target language. For instance, GHC compiler 
transforms Haskell programs into native machine code. Of course the 
difference between a language such as Haskell and assembly is very 
large, so compilers break up the work into multiple transformation 
passes between several intermediate languages.</p>
<p>How do we know that compilers, such as the GHC compiler, are 
“correct”? That is, how do we know that the meaning we encode in our 
source program is preserved in the compiler-emitted target program?</p>
<p>This is where compiler verification comes into play. The goal of 
compiler verification is to prove that a compiler preserves key 
properties of the program it is transforming. In specific, we are 
concerned with preserving the <em>semantics</em>, or behavior of a program, as well as the <em>equivalence</em> between two programs.</p>
<h2 id="semantics-preservation">Semantics preservation</h2>
<p>A semantics preserving transformation from a program <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi></mrow></math>, of source language <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi></mrow></math>, to a program <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi></mrow></math>, of target language <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>T</mi></mrow></math>, ensures that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi></mrow></math> behaves as prescribed by the semantics of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi></mrow></math>. This means that we expect the same observable behavior from running <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi></mrow></math> that we get by running <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi></mrow></math>. Formally, the transformation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>⇝</mo></mrow></math> is semantics preserving when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∀</mo><mi>B</mi><mo>.</mo><mi>s</mi><mo>⇓</mo><mi>B</mi><mo>⇔</mo><mi>t</mi><mo>⇓</mo><mi>B</mi></mrow></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi></mrow></math> is an observable behaviour and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>⇝</mo><mi>t</mi></mrow></math>.</p>
<p>Semantics preservation is what one would intuitively expect from a 
compiler: to transform a program while maintaining its observable 
behaviours. The need for equivalence preservation is more subtle, and 
becomes apparent when we start thinking about module compilation and 
linking target-level code that has been compiled from various source 
languages.</p>
<h2 id="equivalence-preservation">Equivalence preservation</h2>
<p>An equivalence preserving transformation ensures that if two source 
components are in some way equivalent in the source language, then their
 target language translations are also equivalent. The canonical notion 
of program equivalence for many applications is <em>observational</em>, or <em>contextual equivalence</em>.
 Two programs are contextually equivalent if no well-typed context can 
make the programs exhibit observably different behaviors.</p>
<p>A non-equivalence preserving transformation would mean that there exists a target context <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mi>T</mi></msub></mrow></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mi>T</mi></msub><mo stretchy="false">[</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">]</mo><msub><mo>≉</mo><mi>T</mi></msub><msub><mi>C</mi><mi>T</mi></msub><mo stretchy="false">[</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy="false">]</mo></mrow></math> given that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>1</mn></msub><msub><mo>≈</mo><mi>S</mi></msub><msub><mi>s</mi><mn>2</mn></msub></mrow></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>⇝</mo><msub><mi>t</mi><mn>1</mn></msub></mrow></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>2</mn></msub><mo>⇝</mo><msub><mi>t</mi><mn>2</mn></msub></mrow></math>.
 Ensuring that such a context can't arise is important for programmers, 
enabling them to work at the level of abstraction provided by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi></mrow></math> and not worry about the mechanics of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>T</mi></mrow></math>.</p>
<p>Let's take an example out of Kennedy's overview of how compiling C# 
to the .NET Intermediate Language is not equivalence preserving [<a href="http://research.microsoft.com/pubs/64043/appsem-tcs.pdf">13</a>]: At the C# level, <code>true</code> and <code>false</code> are the only values that inhabit the type <code>bool</code>. On the other hand, <code>bool</code> and <code>byte</code> are interchangeable at the IL level. Hence, while the <code>WriteLine</code> statement below would never execute at C# level, some tricky IL code could pass <code>NotNot</code> a byte value not representing <code>true</code> (<code>1</code>) or <code>false</code> (<code>0</code>) to execute the <code>WriteLine</code> statement.</p>
<pre class="sourceCode Java"><code class="sourceCode java"><span class="kw">public</span> <span class="dt">void</span> <span class="fu">NotNot</span>(bool b) {
  bool c = !b;
  <span class="kw">if</span> (!c != b) { Console.<span class="fu">WriteLine</span>(<span class="st">"This cannot happen"</span>); }
}</code></pre>
<p>This means that equivalence of the following C# code is not preserved after compilation to the IL.</p>
<pre class="sourceCode Java"><code class="sourceCode java"><span class="kw">public</span> bool <span class="fu">Eq1</span>(bool x, bool y) { <span class="kw">return</span> x==y; }
<span class="kw">public</span> bool <span class="fu">Eq2</span>(bool x, bool y) { <span class="kw">return</span> (x==<span class="kw">true</span>)==(y==<span class="kw">true</span>); }</code></pre>
<h2 id="full-abstraction">Full abstraction</h2>
<p>Traditionally researchers were concerned whether the denotational model of <a href="https://en.wikipedia.org/wiki/Programming_Computable_Functions">Plotkin's PCF</a> was fully abstract. Here “a model is called <em>fully abstract</em>
 if two terms are semantically equal precisely when they are 
observationally equal, i.e., when they may interchange in all programs 
with no difference in observable behaviour.” [<a href="http://surface.syr.edu/cgi/viewcontent.cgi?article=1002&amp;context=lcsmith_other">6</a>]</p>
<p>In the context of compiler verification, the full abstraction of a 
transformation means that it preserves both semantics and equivalence, 
or rather it is equivalence reflecting and preserving.</p>
<p>To show that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>⇝</mo></mrow></math> is fully abstract, we will need to show it is both equivalence <em>preserving</em> and <em>reflecting</em>. Consider <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>⇝</mo><msub><mi>t</mi><mn>1</mn></msub></mrow></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>2</mn></msub><mo>⇝</mo><msub><mi>s</mi><mn>2</mn></msub></mrow></math>:</p>
<p><strong>Equivalence reflection</strong> means that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>t</mi><mn>1</mn></msub><msub><mo>≈</mo><mi>T</mi></msub><msub><mi>t</mi><mn>2</mn></msub></mrow></math> implies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>1</mn></msub><msub><mo>≈</mo><mi>S</mi></msub><msub><mi>s</mi><mn>2</mn></msub></mrow></math>.
 Reflection captures the notion of compiler correctness, or the 
preservation of semantics. That is, a compiler translation is incorrect 
if it maps non-equivalent source terms to equivalent target terms.</p>
<p><strong>Equivalence preservation</strong> means that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>1</mn></msub><msub><mo>≈</mo><mi>S</mi></msub><msub><mi>s</mi><mn>2</mn></msub></mrow></math> implies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>t</mi><mn>1</mn></msub><msub><mo>≈</mo><mi>T</mi></msub><msub><mi>t</mi><mn>2</mn></msub></mrow></math>. Preservation is harder to prove than reflection, hence it is often proven indirectly by assuming <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow></math> are not equivalent and deriving a contradiction.</p>
<p>Note that if the set of possible contexts at the target level is 
restricted to those that can be obtained by translating source contexts,
 then proving full abstraction becomes easy since equivalence 
preservation falls out of proving equivalence reflection. Such a 
restriction would rule out the ability to reason about most modern 
languages, for instance compiled Java code is often linked with code 
from other source languages at the JVM byte-code level.</p>
<h2 id="how-do-you-verify-compilers">How do you verify compilers?</h2>
<p>There are two main approaches to compiler verification: logical relations and bisimulation.</p>
<p><strong>Logical relations</strong> are a tool for generalizing, or 
strengthening an induction hypothesis as a means to make a proof go 
through. They can be based on either denotational or operational 
semantics, and the basic idea behind them is to define relations on 
well-typed terms via structural induction on the syntax of types. 
Logical relations were originally introduced to prove strong 
normalization of the simply typed <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi></mrow></math>-calculus
 but have since been adapted to work with various exotic language 
features. For instance, step-indexing allows for reasoning about 
non-terminating languages [<a href="ftp://ftp.cs.princeton.edu/reports/2000/629.pdf">5</a>] and indexing with Kripke worlds allows for reasoning about mutable references [<a href="http://www.ccs.neu.edu/home/amal/papers/sdri.pdf">7</a>].</p>
<p><strong>Bisimulations</strong>, roughly put, are relations over the operational semantics of source and target languages. They “define a relation <em>R</em> between pairs of configurations, such that if two configurations are related by <em>R</em>, and each takes a step, then the resulting configurations are related by <em>R</em>, and if two final configurations are related by <em>R</em>, then they represent the same answer” [<a href="ftp://ftp.ccs.neu.edu/pub/people/wand/papers/popl-06.pdf">2</a>]. According to Dreyer et al. [<a href="http://www.mpi-sws.org/~dreyer/papers/stslr/journal.pdf">9</a>] there are two main types of bisimulations used for equivalence related compiler verification: environmental bisimulations [<a href="ftp://ftp.ccs.neu.edu/pub/people/wand/papers/popl-06.pdf">2</a>, <a href="https://dl.acm.org/citation.cfm?id=1889997.1890002">3</a>, <a href="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1155&amp;context=cis_papers">12</a>] and normal form bisimulations [<a href="http://www.diku.dk/~stovring/papers/complete-theory.pdf">4</a>].</p>
<p>Both techniques have their advantages and disadvantages, though I'll 
leave deeper explorations of these techniques to a future post.</p>
<h2 id="closed-vs.open-world-compilers">Closed vs. Open world compilers</h2>
<p>One important distinction when verifying a compiler is whether to assume a closed or open world.</p>
<p>With a <strong>closed world</strong>, compilers must have access to entire, closed programs, which includes linked libraries. Projects like <a href="http://compcert.inria.fr/doc/index.html">CompCert</a>
 utilize this closed world assumption to do compiler verification. In 
the case of CompCert, a simulation proof is set up, where the source and
 compiled programs are run and deemed semantically equivalent if they 
produce the same trace of observable events [<a href="http://gallium.inria.fr/~xleroy/publi/compcert-CACM.pdf">10</a>].
 Often while dealing with compilers in the real world, we cannot assume a
 closed world. For instance, it would be extremely useful to be able to 
verify that the compilation of an unlinked library is correct, which 
does not fit in a closed world.</p>
<p>On the other hand, an <strong>open world</strong> enables compilers 
to work with open programs that can then later be linked together. In 
order to work in an open world, Benton and Hurr [<a href="http://www.mpi-sws.org/~gil/publications/realize.pdf">8</a>]
 set up a cross-language logical relation to prove semantics 
preservation of a transformation from a purely functional language to 
the <a href="https://en.wikipedia.org/wiki/SECD_machine">SECD machine</a>. Using their technique, given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>1</mn></msub><mo>⇝</mo><msub><mi>t</mi><mn>1</mn></msub></mrow></math>, in order to prove the correctness of the linking of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow></math> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow></math>, one must provide a corresponding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>2</mn></msub></mrow></math> and show <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>2</mn></msub><mo>≈</mo><msub><mi>t</mi><mn>2</mn></msub></mrow></math>. Given a large <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow></math>, the requirement to provide a corresponding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>2</mn></msub></mrow></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>2</mn></msub><mo>⇝</mo><msub><mi>t</mi><mn>2</mn></msub></mrow></math> becomes too onerous for this proof technique to be practically applicable.</p>
<p>This brings us to exciting new research by Perconti and Ahmed that attempts to remedy the need to provide a corresponding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>s</mi><mn>2</mn></msub></mrow></math> by utilizing interoperability semantics (originally introduced by Matthews and Findler [<a href="http://users.eecs.northwestern.edu/~robby/pubs/papers/popl2007-mf-color.pdf">11</a>]). As presented by Perconti at <a href="http://hope2012.mpi-sws.org/">HOPE 12</a>, they are attempting to prove semantics preservation of a ML to assembly compiler by embedding source and target languages, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi></mrow></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>T</mi></mrow></math>, into a larger language, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>T</mi></mrow></math> (in reality they define multiple such languages for each compiler pass). Corresponding semantic boundary wrappings (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext mathvariant="bold">ST</mtext></mrow></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext mathvariant="bold">TS</mtext></mrow></math>) are provided, enabling a wrapped <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext mathvariant="bold">TS</mtext><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></math> term to be used as a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi></mrow></math> term in proofs. Their technique for proving semantics preservation is then to use a logical relation over the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>T</mi></mrow></math> language to prove contextual equivalence between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi></mrow></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext mathvariant="bold">ST</mtext><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></math> terms.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post we've covered what properties one looks for when 
verifying a compiler. Semantics preservation, also known as compiler 
correctness, refers to a compiler's ability to preserve the meaning of a
 program throughout a transformation. Equivalence preservation ensures 
that programmers can think in terms of source level abstractions, and 
not worry about how modules and libraries interact at the target level. A
 compiler that exhibits both properties can be said to be fully 
abstract.</p>
<p>From there, I briefly introduced to common techniques for compiler verification: logical relations and bisimulations.</p>
<p>Lastly, I covered the difference between closed and open world 
compiler verifications and alluded to some ongoing research on open 
world compilers.</p>
<p>All of this brings many more questions and things to explore:</p>
<ul>
<li>I'd like to better understand bisimulation techniques and am curious whether they scale to open world compilers?</li>
<li>Logical relations can have some pretty crazy formulations, for instance the logical relation presented in [<a href="http://www.mpi-sws.org/~dreyer/papers/stslr/journal.pdf">9</a>] is indexed by a state transition systems. How else can logical relations be indexed?</li>
<li>The interoperability semantics technique used by Perconti and Ahmed, as well as in previous work [<a href="http://www.ccs.neu.edu/home/amal/papers/epc.pdf">1</a>], seems too good to be true. How do interoperability wrappers eliminate the need for the decompilation of a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>T</mi></mrow></math> term into an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi></mrow></math> term?</li>
<li>Are there other useful notions of equivalence for compiler verification other than observational equivalence?</li>
</ul>
<h2 id="links">Links</h2>
<ul>
<li>[1] “<a href="http://www.ccs.neu.edu/home/amal/papers/epc.pdf">An Equivalence-Preserving CPS Translation via Multi-Language Semantics</a>” by Ahmed and Blume 2011</li>
<li>[2] “<a href="ftp://ftp.ccs.neu.edu/pub/people/wand/papers/popl-06.pdf">Small bisimulations for reasoning about higher-order imperative programs</a>” by Koutavas and Wand 2006</li>
<li>[3] “<a href="https://dl.acm.org/citation.cfm?id=1889997.1890002">Environmental bisimulations for higher-order languages</a>” by Sangiorgi et al. 2011</li>
<li>[4] “<a href="http://www.diku.dk/~stovring/papers/complete-theory.pdf">A complete, co-inductive syntactic theory of sequential control and state</a>” by Størvig and Lassen 2007</li>
<li>[5] “<a href="ftp://ftp.cs.princeton.edu/reports/2000/629.pdf">An indexed model of recursive types for foundationalproof-carrying code</a>” by Appel and McAllester 2001</li>
<li>[6] “<a href="http://surface.syr.edu/cgi/viewcontent.cgi?article=1002&amp;context=lcsmith_other">Kripke Logical Relations and PCF</a>” by O'Hearn and Riecke 1994</li>
<li>[7] “<a href="http://www.ccs.neu.edu/home/amal/papers/sdri.pdf">State-dependent representation independence</a>” by Ahmed et al. 2009</li>
<li>[8] “<a href="http://www.mpi-sws.org/~gil/publications/realize.pdf">Biorthogonality, Step-Indexing and Compiler Correctness</a>” by Benton and Hur 2009</li>
<li>[9] “<a href="http://www.mpi-sws.org/~dreyer/papers/stslr/journal.pdf">The impact of higher-order state and control effects on local relational reasoning</a>” by Dreyer, Neis and Birkedal 2012</li>
<li>[10] “<a href="http://gallium.inria.fr/~xleroy/publi/compcert-CACM.pdf">Formal verification of a realistic compiler</a>” by Leroy 2008</li>
<li>[11] “<a href="http://users.eecs.northwestern.edu/~robby/pubs/papers/popl2007-mf-color.pdf">Operational Semantics for Multi-Language Programs</a>” by Matthews and Findler 2007</li>
<li>[12] “<a href="http://repository.upenn.edu/cgi/viewcontent.cgi?article=1155&amp;context=cis_papers">A bisimulation for type abstraction and recursion</a>” by Sumii and Pierce 2007</li>
<li>[13] “<a href="http://research.microsoft.com/pubs/64043/appsem-tcs.pdf">Securing the .NET Programming Model</a>” by Kennedy 2006</li>
<li>“<a href="http://www.mpi-sws.org/~dreyer/tor/index.php">Typed Operational Reasoning</a>” Derek Dreyer's course on Logical Relations</li>
</ul></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://ccs.neu.edu/home/mates/articles/compiling-with-conts.html"><span xml:base="http://www.ccs.neu.edu/home/mates/articles.atom">Modeling "The Essence of Compiling with Continuations"</span></a><div class="lastUpdated">December 6, 2012, 1:00 AM</div></h3><div xml:base="http://www.ccs.neu.edu/home/mates/articles.atom" class="feedEntryContent"><p>Understanding
 technical papers is difficult. It is easy to convince yourself that you
 understand a paper, but even easier to shatter this illusion by 
attempting to run through the paper’s figures by hand. Figures often 
contain subtleties, omissions, and unfortunately typos that need to be 
digested and worked through.</p>
<p>This became apparent to me while working through the classic paper “<a href="https://dl.acm.org/citation.cfm?id=155113">The Essence of Compiling with Continuations</a>” for <a href="http://www.ccs.neu.edu/home/matthias/">Matthias Felleisen</a>'s <a href="http://www.ccs.neu.edu/home/matthias/7400-f12/">Principles of Programming Languages</a> course. As a means to better comprehend this paper, my project partner <a href="http://www.ccs.neu.edu/home/ejs/">Erik Silkensen</a> and I mechanically modeled the paper's figures in <a href="http://redex.racket-lang.org/">Redex</a>.</p>
<p>In this post I will present the paper and link to the corresponding executable models that can be <a href="https://github.com/esilkensen/cwc">found on GitHub</a> as I go.</p>
<h2 id="continuation-passing-style-cps">Continuation passing style (CPS)</h2>
<p>Compilers transform high level source languages, such as Java or 
Haskell into lower level target languages, such as JVM or LLVM. 
Intermediate language forms are often used to enable the application of 
generalized optimizations. A popular intermediate form is continuation 
passing style (CPS), in which procedures don't return but pass their 
result to a caller-provided continuation or call-back function. Take for
 instance, a function <code>add1 = λ x . (+ x 1)</code>, in CPS it would look like <code>add1' = λ x k . (k (+ x 1))</code>, where <code>k</code>
 is the provided continuation. Aside from enabling many optimizations, 
CPS makes control flow of programs explicit and is easy to translate 
into assembly. For more intuition on CPS, Matt Might has several 
excellent articles on <a href="http://matt.might.net/articles/by-example-continuation-passing-style/">CPS by example</a>, <a href="http://matt.might.net/articles/cps-conversion/">compiling with CPS</a> and <a href="http://matt.might.net/articles/programming-with-continuations--exceptions-backtracking-search-threads-generators-coroutines/">implementing language features using CPS</a>.</p>
<h2 id="the-essence-of-compiling-with-continuations">The Essence of Compiling with Continuations</h2>
<p>An optimizing CPS transformation usually takes multiple passes. Flanagan et. al. show in “<a href="https://dl.acm.org/citation.cfm?id=155113">The Essence of Compiling with Continuations</a>”
 that one can forgo a standard 3-pass CPS transformation while still 
capturing the essence of compiling with continuations by doing a single 
source level transformation to A-Normal Form (ANF).</p>
<p>To show this, the authors present a Scheme-like language CS, CPS 
convert it into the CPS(CS) language. Then they incrementally optimize 
an abstract machine that operates over the CPS(CS) language, arriving at
 the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>c</mi><mi>p</mi><mi>s</mi></mrow></msub><mi>E</mi><mi>K</mi></mrow></math> machine. To close they prove its equivalence to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mi>a</mi></msub><mi>E</mi><mi>K</mi></mrow></math> machine, which operates over ANF programs in the A(CS) language.</p>
<p>Below is the initial Scheme-like <a href="https://github.com/esilkensen/cwc/blob/master/src/cs.rkt#L11">CS Language</a>, which has expressions <code>M</code>, values <code>V</code> and operations <code>O</code>.</p>
<pre><code>M ::= V | (let (x M) M) | (if0 M M M) | (M M ...) | (O M ...)
V ::= number | x | (λ (x ...) M)
O ::= + | - | * | /</code></pre>
<h2 id="cps-transformations">CPS transformations</h2>
<p>To convert the CS language into CPS, we begin with the <a href="https://github.com/esilkensen/cwc/blob/master/src/cps.rkt#L62">naive CPS translation F</a> that adds continuation to terms in the language.</p>
<pre><code>F[V]               = λ k . (k Φ[V])
F[(let (x M1) M2)] = λ k . (F[M1] (λ t . (let (x t) (F[M2] k))))
F[(if0 M1 M2 M3)]  = λ k . (F[M1] (λ t . (if0 t (F[M2] k) (F[M3] k))))
F[(M M1)]          = λ k . (F[M] (λ t . (F[M1] (λ t1 . (t k t1)))))
F[(0 M)]           = λ k . (F[M] (λ t . (O' k t)))))</code></pre>
<pre><code>Φ[c]       = c
Φ[x]       = x
Φ[λ x . M] = λ k x . (F[M] k)</code></pre>
<p>It is naive because it introduces many administrative <code>λ</code> terms that can be simplified away by applying β-reductions. For instance, <code>F(λ x . x)</code> will result in:</p>
<pre><code>(λ k3 . (k3 (λ k2 x . ((λ k1 . ((λ k . (k x)) (λ t . (t k1)))) k2))))</code></pre>
<p>which can then be simplified to:</p>
<pre><code>(λ k3 . (k3 (λ k2 x . (x k2))))</code></pre>
<p>by applying β-reductions to <code>λ</code>s introduced by the <code>F</code> function wherever possible.</p>
<p>The resulting language after applying the <code>F</code> function to CS terms is <a href="https://github.com/esilkensen/cwc/blob/master/src/cps.rkt#L11">CPS(CS)</a>, where there are expressions <code>P</code>, values <code>W</code> and the CPS equivalent of operators found in CS <code>O'</code>:</p>
<pre><code>P  ::= (k W) | (let (x W) P) | (if0 W P P)
     | (W k W ...) | (W (λ (x) P) W ...)
     | (O' k W ...) | (O' (λ (x) P) W ...)
W  ::= number | x | (λ (k x ...) P)
O' ::= +' | -' | *' | /'</code></pre>
<h2 id="optimizing-abstract-machines-for-cpscs">Optimizing abstract machines for CPS(CS)</h2>
<p>Continuations are explicitly present in terms of the CPS(CS) 
language, hence control and environment are the only components needed 
to create an abstract machine capable of evaluating CPS(CS) programs. 
The authors begin with such a machine, the <a href="https://github.com/esilkensen/cwc/blob/master/src/cps-ce.rkt#L27"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>c</mi><mi>p</mi><mi>s</mi></mrow></msub><mi>E</mi></mrow></math></a>
 machine, but quickly notice that continuation variables are often 
redundantly bound in the environment. There is no need when evaluating <code>((λ k1 x . P) k2 y)</code> to bind <code>k1 := k2</code> in the environment since they point to the same continuation. Addressing this, a control stack is added, resulting in a <a href="https://github.com/esilkensen/cwc/blob/master/src/cps-cek.rkt#L28"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>c</mi><mi>p</mi><mi>s</mi></mrow></msub><mi>E</mi><mi>K</mi></mrow></math></a>
 machine in which continuations are no longer stored in the state's 
environment. From there they notice that since the machine is keeping 
track of the program's continuations, continuation variables found in 
CPS(CS) terms are not used to lookup continuations in the environment. 
Hence an unCPS transformation is designed to remove explicit 
continuations from terms and a subsequent <a href="https://github.com/esilkensen/cwc/blob/master/src/a-cek.rkt#L28"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mi>a</mi></msub><mi>E</mi><mi>K</mi></mrow></math></a> machine is formulated.</p>
<h2 id="uncps">UnCPS</h2>
<p>In the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>c</mi><mi>p</mi><mi>s</mi></mrow></msub><mi>E</mi></mrow></math> machine, the continuation variables found in terms are used to lookup continuations in the environment. Migrating to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>c</mi><mi>p</mi><mi>s</mi></mrow></msub><mi>E</mi><mi>K</mi></mrow></math>
 machine means that these continuations are already managed by the 
state's control stack. This leaves continuation variables in terms 
useless, so let's drop them. What about the remaining terms that have 
with continuations of the form <code>λ x . P</code>? These can be transformed into <code>let</code> expressions by binding the current computation to the continuation's free <code>x</code> and putting the continuation body in the body of the <code>let</code>:</p>
<pre><code>(W (λ x . P) W ...) → (let (x (W W ...)) P)</code></pre>
<p>Doing so means that continuations are no longer passed around but 
become the surrounding context, syntactically linearizing the program. 
This continuation removal transformation can be formalized as the <a href="https://github.com/esilkensen/cwc/blob/master/src/a.rkt#L31"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi></mrow></math> metafunction</a> found in our Redex models. For instance applying <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi></mrow></math> to the following CPS expression:</p>
<pre><code>(+' (λ t1 . (let (x 1) (f (λ t2 . (+' k t1 t2)) x))) 2 2)</code></pre>
<p>results in the following CS expression:</p>
<pre><code>(let (t1 (+ 2 2))
  (let (x 1)
    (let (t2 (f x)))
      (+ t1 t2)))</code></pre>
<p>The range of this function is the A(CS) language, which is a stricter subset of the CS language.</p>
<h2 id="a-normal-form-anf-language">A-Normal Form (ANF) language</h2>
<pre><code>M ::= V | (let (x V) M) | (if0 V M M)
    | (V V ...) | (let (x (V V ...)) M)
    | (O V ...) | (let (x (O V ...)) M)
V ::= number | x | (λ (x ...) M)</code></pre>
<p>The <a href="https://github.com/esilkensen/cwc/blob/master/src/a.rkt#L12">A(CS) language</a>
 that results from unCPSing the CPS(CS) language holds some interesting 
properties. It maintains the sequential structure of CPS code 
implicitly, without continuations. Programs in the A(CS) language are 
constrained such that non-value <code>M</code> terms (computations) must
 be let-bound or appear in tail position. These properties enable 
certain optimizations, such as being able to write abstract machines 
that take larger steps.</p>
<h2 id="direct-anf-translation">Direct ANF translation</h2>
<p>So far, in order to translate into the A(CS) language we've had to convert to CPS, simplify administrative <code>λ</code>s, and translate out of CPS via the unCPS function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi></mrow></math>.
 One must wonder, can we forgo CPS completely and translate CS to A(CS) 
in one source-level pass? Flanagan et. al. show this is possible by 
presenting a strongly normalizing A-reduction relation which takes CS 
terms to A(CS) terms using the evaluation context <code>ε</code>. The evaluation context <code>ε</code> is defined such that the holes represent the next reducible expression according to the original CS language's <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>C</mi><mi>S</mi></mrow></msub><mi>E</mi><mi>K</mi></mrow></math> machine. For instance, when the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>C</mi><mi>S</mi></mrow></msub><mi>E</mi><mi>K</mi></mrow></math> machine sees a term <code>(if0 (- 1 2) 2 3)</code> it will reduce <code>(- 1 2)</code> first.</p>
<pre><code>ε ::= [] | (let (x ε) M) | (if0 ε M M) | (F V ... ε M ...)
      where F = V or F = O</code></pre>
<p>The idea is to create an reduction relation, A-Reduction, where we 
bind intermediate computations found in context holes to variables and 
replace these computations with their binding variables. This results in
 programs with a linearized control flow that only require one 
continuation structure in the corresponding abstract machine 
implementation (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mi>a</mi></msub><mi>E</mi><mi>K</mi></mrow></math>).</p>
<p>A-Reduction:</p>
<pre><code>(A1)  ε[(let (x M) N)] →a (let (x M) ε[N])
                          where ε ≠ [] and x ∉ FreeVariables(ε)
(A2)  ε[(if0 V M1 M2)] →a (if0 V ε[M1] ε[M2])
(A3)  ε[(F V ...)]     →a (let (t (F V ...)) ε[t])
                          where F = V or F = O, ε ≠ [], t ∉ FreeVariables(ε),
                                ∀ ε' . ε ≠ ε'[(let (z []) M)]</code></pre>
<p>The A-Reduction is comprised of 3 rules:</p>
<ul>
<li>(A1) moves let-bindings out of the current context, as long as we make sure to pick a fresh <code>x</code> and substitute it in <code>N</code>. This rule helps us lift <code>let</code> terms out of computational locations so that we no longer need the <code>lt</code> continuation found in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>C</mi><mi>S</mi></mrow></msub><mi>E</mi></mrow></math> machine.</li>
<li>(A2) moves <code>if</code> terms out of the current context, duplicating the context for each branch. This rule enables us to get ride of the <code>if</code> continuation found in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>C</mi><mrow><mi>C</mi><mi>S</mi></mrow></msub><mi>E</mi></mrow></math> machine.</li>
<li>(A3) binds the result of intermediate computations to variables and 
lifts them out of the current context. We need to make sure that the 
hole in <code>ε</code> isn't of the shape <code>ε'[(let (x []) M)]</code> for some arbitrary <code>ε'</code>,
 or else we would be unnecessarily be introducing let-bindings. This 
rule ensures that applications are only found in the body of <code>let</code> expressions or in tail position.</li>
</ul>
<p>All the rules enforce the side-condition that <code>ε</code> isn't empty because if it was the reduction relation would never terminate.</p>
<p>The A-Reduction is a reduction relation over the entire program term, and it turns out that once we apply any of the <code>→a</code> rules, we are left with a term that can't be matched by the <code>ε</code> context. This is due to the fact that the <code>ε</code> context only matches terms not yet in ANF and the <code>→a</code>
 rules introduce terms in ANF. In order to model this in Redex, we were 
forced to add a ψ context to let us match over terms already in ANF:</p>
<pre><code>ψ ::= [] | (let (x V) ψ) | (let (x (F V ...)) ψ) | (if0 V ψ M) | (if0 V M ψ)
      where F = V or F = O</code></pre>
<p>The A-Reduction rules can then be wrapped in a ψ context, allowing 
for terms already in ANF to be put in the ψ context so reductions on 
non-ANF terms can be made. This results in the <a href="https://github.com/esilkensen/cwc/blob/master/src/a.rkt#L57">Adapted A-Reduction</a> which we were able to model in a straight forward manner in Redex:</p>
<pre><code>ψ[M1] → ψ[M2] when M1 →a M2</code></pre>
<h2 id="typos">Typos</h2>
<p>The typos we found in “<a href="https://dl.acm.org/citation.cfm?id=155113">The Essence of Compiling with Continuations</a>” are quite minor, the only reason I mention them is to motivate Redex's LaTeX exporting features.</p>
<p>In the CPS transformation figure of the paper (Figure 3), the auxiliary <code>Φ</code> function reads</p>
<pre><code>Φ[λ x . M] = λ k x . F[M]</code></pre>
<p>failing to use the <code>k</code> variable bound by the <code>λ</code>, and should hence read</p>
<pre><code>Φ[λ x . M] = λ k x . (F[M] k)</code></pre>
<p>Additionally the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>U</mi></mrow></math> function calls an auxiliary <code>Ψ</code> function that unCPSs values. The <code>λ</code> case of <code>Ψ</code> reads <code>Ψ[λ k x . P] = λ x . U[M]</code> but clearly should be <code>Ψ[λ k x . P] = λ x . U[P]</code>.</p>
<p>If Redex was available in 1993, such typos could have have been 
avoided by creating Redex models and then exporting them as LaTeX 
figures, in the flavor of <a href="http://users.eecs.northwestern.edu/~robby/lightweight-metatheory/">Run Your Research</a>.</p>
<h2 id="resources-and-further-reading">Resources and Further Reading</h2>
<ul>
<li><a href="https://github.com/esilkensen/cwc">Our Redex Models on GitHub</a></li>
<li>Matt Might’s blog post on <a href="http://matt.might.net/articles/a-normalization/">A Normalization</a></li>
<li>Matt Might’s blog posts on CPS: <a href="http://matt.might.net/articles/by-example-continuation-passing-style/">CPS by example</a>, <a href="http://matt.might.net/articles/cps-conversion/">compiling with CPS</a> and <a href="http://matt.might.net/articles/programming-with-continuations--exceptions-backtracking-search-threads-generators-coroutines/">implementing language features using CPS</a>.</li>
<li>The paper: “<a href="https://dl.acm.org/citation.cfm?id=155113">The Essence of Compiling with Continuations</a>”</li>
<li>A preceding paper that introduces unCPS: “<a href="https://dl.acm.org/citation.cfm?id=141563">Reasoning about programs in continuation-passing style</a>”</li>
<li><a href="http://www.amazon.com/Semantics-Engineering-Redex-Matthias-Felleisen/dp/0262062755/">Semantics Engineering With PLT Redex</a> textbook</li>
</ul></div></div><div style="clear: both;"></div><div class="entry"><h3><a href="http://ccs.neu.edu/home/mates/articles/monads.html"><span xml:base="http://www.ccs.neu.edu/home/mates/articles.atom">Monads: How do they work?</span></a><div class="lastUpdated">March 5, 2012, 1:00 AM</div></h3><div xml:base="http://www.ccs.neu.edu/home/mates/articles.atom" class="feedEntryContent"><p>I volunteered to give a presentation on monads for the <a href="http://www.eng.utah.edu/~cs6510/">Practical Functional Programming</a> course at the <a href="http://www.utah.edu/">U of U</a>.
 This post is a product of me preparing for that talk. It is a general 
overview of monads, as well as what types of problems they can solve. 
I'm not concerned about providing anything novel here, in fact most of 
the content found here is a rehash of other online resources (see <em>Further Reading</em> section).</p>
<p>For readers interested in gaining a solid understanding of monads, I highly recommend trudging through chapters <a href="http://book.realworldhaskell.org/read/monads.html">14</a>, <a href="http://book.realworldhaskell.org/read/programming-with-monads.html">15</a>, and <a href="http://book.realworldhaskell.org/read/monad-transformers.html">18</a> of <a href="http://book.realworldhaskell.org/">Real World Haskell</a>.</p>
<h2 id="motivation-for-monads">Motivation for Monads</h2>
<p>How do you code up doubly nested for-loops in a purely functional 
language? Is there an elegant way to pass around program state without 
explicitly threading it in and out of every function? How do you encode 
sequential actions, such as reading and writing files?</p>
<p>It turns out that all of these problems can be solved using the monad abstraction.</p>
<h2 id="what-is-a-monad">What is a Monad?</h2>
<p>Monads are a general way to encode sequential actions. I like to 
think of them as containers that wrap values of a given type and expose a
 framework enabling action composition.</p>
<p>When we work with monads we want to be able to wrap values in containers (<code>return</code>), as well as compose together containers (<code>bind</code> or <code>&gt;&gt;=</code>).</p>
<p>In Haskell code, a monad is a typeclass. Typeclasses are a way of defining ad hoc polymorphism.</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- Monad Typeclass</span>
<span class="kw">class</span> <span class="dt">Monad</span> m <span class="kw">where</span>
    <span class="co">-- Chain together containers</span>
<span class="ot">    (&gt;&gt;=) ::</span> m a <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> m b) <span class="ot">-&gt;</span> m b

    <span class="co">-- Inject value into a container</span>
<span class="ot">    return ::</span> a <span class="ot">-&gt;</span> m a

    <span class="co">-- additional helper functions that aren't necessary:</span>
    <span class="co">-- &gt;&gt; is like &gt;&gt;= but throws away the first result</span>
<span class="ot">    (&gt;&gt;) ::</span> m a <span class="ot">-&gt;</span> m b <span class="ot">-&gt;</span> m b

    <span class="co">-- fail is a technical necessity used for pattern match failures</span>
    <span class="co">-- in do notation</span>
<span class="ot">    fail ::</span> <span class="dt">String</span> <span class="ot">-&gt;</span> m a</code></pre>
<p>If we want to make a type an instance of the <code>Monad</code> typeclass, we just have to define <code>&gt;&gt;=</code> and <code>return</code> for that type. Let's try it out with a data type you may have seen before.</p>
<h2 id="maybe-monad">Maybe Monad</h2>
<p>Did you know that the <code>Maybe</code> datatype is a monad used for anonymous exception handling? Let's define it and its monadic operators here:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Maybe</span> a <span class="fu">=</span> <span class="dt">Nothing</span>
             <span class="fu">|</span> <span class="dt">Just</span> a

<span class="kw">instance</span> <span class="dt">Monad</span> <span class="dt">Maybe</span> <span class="kw">where</span>
    <span class="co">-- bind together operations</span>
    <span class="dt">Just</span> x <span class="fu">&gt;&gt;=</span> k  <span class="fu">=</span>  k x
    <span class="dt">Nothing</span> <span class="fu">&gt;&gt;=</span> _ <span class="fu">=</span>  <span class="dt">Nothing</span>

    <span class="co">-- inject value</span>
    return x      <span class="fu">=</span>  <span class="dt">Just</span> x

    <span class="co">-- then</span>
    <span class="dt">Just</span> _ <span class="fu">&gt;&gt;</span> k   <span class="fu">=</span>  k
    <span class="dt">Nothing</span> <span class="fu">&gt;&gt;</span> _  <span class="fu">=</span>  <span class="dt">Nothing</span>

    fail _        <span class="fu">=</span>  <span class="dt">Nothing</span></code></pre>
<p>How does extending <code>Maybe</code> to be an instance of the <code>Monad</code> typeclass make our lives easier? Let's take a look at the <code>animalColorLookup</code> function, which doesn't utilize <code>Maybe</code> as a monad:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="ot">animalFriends ::</span> [(<span class="dt">String</span>, <span class="dt">String</span>)]
animalFriends <span class="fu">=</span> [ (<span class="st">"Pony"</span>, <span class="st">"Lion"</span>)
                , (<span class="st">"Lion"</span>, <span class="st">"Manticore"</span>)
                , (<span class="st">"Unicorn"</span>, <span class="st">"Lepricon"</span>)
                ]

<span class="co">-- Explicitly chaining Maybes to find ponys friends friends friend</span>
<span class="ot">animalFriendLookup ::</span> [(<span class="dt">String</span>, <span class="dt">String</span>)] <span class="ot">-&gt;</span> <span class="dt">Maybe</span> <span class="dt">String</span>
animalFriendLookup animalMap <span class="fu">=</span>
  <span class="kw">case</span> lookup <span class="st">"Pony"</span> animalMap <span class="kw">of</span>
       <span class="dt">Nothing</span> <span class="ot">-&gt;</span> <span class="dt">Nothing</span>
       <span class="dt">Just</span> ponyFriend <span class="ot">-&gt;</span>
         <span class="kw">case</span> lookup ponyFriend animalMap <span class="kw">of</span>
              <span class="dt">Nothing</span> <span class="ot">-&gt;</span> <span class="dt">Nothing</span>
              <span class="dt">Just</span> ponyFriendFriend <span class="ot">-&gt;</span>
                <span class="kw">case</span> lookup ponyFriendFriend animalMap <span class="kw">of</span>
                     <span class="dt">Nothing</span> <span class="ot">-&gt;</span> <span class="dt">Nothing</span>
                     <span class="dt">Just</span> friend <span class="ot">-&gt;</span> <span class="dt">Just</span> friend</code></pre>
<p>The chaining of <code>lookup</code> statements wrapped in <code>case</code> gets out of hand quickly. To remedy this we can make use of the <code>bind</code> (<code>&gt;&gt;=</code>) operator defined by the <code>Maybe</code> monad along with some anonymous functions to writer a cleaner version:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- Use Bind to chain lookups</span>
<span class="ot">monadicAnimalFriendLookup ::</span> [(<span class="dt">String</span>, <span class="dt">String</span>)] <span class="ot">-&gt;</span> <span class="dt">Maybe</span> <span class="dt">String</span>
monadicAnimalFriendLookup animalMap <span class="fu">=</span>
      lookup <span class="st">"Pony"</span> animalMap
  <span class="fu">&gt;&gt;=</span> (\ponyFriend <span class="ot">-&gt;</span> lookup ponyFriend animalMap
  <span class="fu">&gt;&gt;=</span> (\ponyFriendFriend <span class="ot">-&gt;</span> lookup ponyFriendFriend animalMap
  <span class="fu">&gt;&gt;=</span> (\friend <span class="ot">-&gt;</span> <span class="dt">Just</span> friend)))</code></pre>
<p>Lastly, most Haskellers are used to seeing <code>do</code> blocks when dealing with monads. <code>do</code> blocks are syntactic sugar to make monadic code look cleaner, as well as giving it an imperative feel.</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- Use Do-Block sugar magic</span>
<span class="ot">sugaryAnimalFriendLookup ::</span> [(<span class="dt">String</span>, <span class="dt">String</span>)] <span class="ot">-&gt;</span> <span class="dt">Maybe</span> <span class="dt">String</span>
sugaryAnimalFriendLookup animalMap <span class="fu">=</span> <span class="kw">do</span>
  ponyFriend <span class="ot">&lt;-</span> lookup <span class="st">"Pony"</span> animalMap
  ponyFriendFriend <span class="ot">&lt;-</span> lookup ponyFriend animalMap
  friend <span class="ot">&lt;-</span> lookup ponyFriendFriend animalMap
  return friend</code></pre>
<p>While <code>do</code> block syntax is nice, it sugar coats a lot 
details that monad beginners should be exposed to. Hence, I'm going to 
stick with the more explicit syntax for now, but later on I'll explain 
how <code>do</code> block code is desugared.</p>
<p>The <code>Maybe</code> monad offers some nice abstractions to clean 
up code, but let's explore some examples that really show off the power 
of monadic abstractions.</p>
<h2 id="threading-state">Threading State</h2>
<p>Say you want to code up some sort of abstract syntax tree (AST) 
transformation program that traverses an AST, inserting unique symbols 
every once and a while.</p>
<p>To do this in a pure setting, one must pass a counter in and out of every function call that needs to create unique symbols:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- for simplicity we'll use a string instead of an Abstract Data Type for ASTs</span>
<span class="kw">type</span> <span class="dt">Sexpr</span> <span class="fu">=</span> <span class="dt">String</span>

<span class="co">-- Add unique symbol to Sexpr using naive threading of program state</span>
<span class="ot">transformStmt ::</span> <span class="dt">Sexpr</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> (<span class="dt">Sexpr</span>, <span class="dt">Int</span>)
transformStmt expr counter <span class="fu">=</span> (newExpr, newCounter)
  <span class="kw">where</span> newExpr <span class="fu">=</span> <span class="st">"(define "</span> <span class="fu">++</span> uniqVarName <span class="fu">++</span> <span class="st">" "</span> <span class="fu">++</span> expr <span class="fu">++</span> <span class="st">")"</span>
        newCounter <span class="fu">=</span> counter <span class="fu">+</span> <span class="dv">1</span>
        uniqVarName <span class="fu">=</span> <span class="st">"tmpVar"</span> <span class="fu">++</span> (show counter)</code></pre>
<p>This is fine, but there is potential for a lot of boilerplate. In 
addition, if we decide we also need to pass around an environment set, 
we'll have to manually change all of our function type signatures.</p>
<p>This problem is screaming to be abstracted, so let's see if monads help…</p>
<p><strong>Like the Internet, Monads are a series of pipes</strong></p>
<p>It turns out we can generalize the threading of state seen in <code>transformStmt</code>. Let's chop off <code>Int -&gt; (Sexpr, Int)</code> from the <code>transformStmt</code> type signature and replace it with the <code>State</code> type constructor defined below. This leaves us with <code>transformStmt :: Sexpr -&gt; State Int Sexpr</code>.
 They are of the exact same type but we've used the type constructor to 
abstract away the fact that we take in a state value and output a state 
value.</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- State type constructor with the runState record syntax</span>
<span class="co">-- to extraction or 'run' the state</span>
<span class="kw">newtype</span> <span class="dt">State</span> s a <span class="fu">=</span> <span class="dt">State</span> {
<span class="ot">      runState ::</span> s <span class="ot">-&gt;</span> (a, s)
    }</code></pre>
<p>Now that we have a type constructor we can start to think about making an instance of the <code>Monad</code> typeclass. With <code>return</code> we want to take a normal value and make it accept a piece of state and return that state along with the original value.</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="kw">instance</span> <span class="dt">Monad</span> (<span class="dt">State</span> s) <span class="kw">where</span>
  <span class="co">-- return :: a -&gt; State s a</span>
  return a <span class="fu">=</span> <span class="dt">State</span> <span class="fu">$</span> \s <span class="ot">-&gt;</span> (a, s)

  <span class="co">-- (&gt;&gt;=) :: State s a -&gt; (a -&gt; State s b) -&gt; State s b</span>
  m <span class="fu">&gt;&gt;=</span> k <span class="fu">=</span> <span class="dt">State</span> <span class="fu">$</span> \s <span class="ot">-&gt;</span> <span class="kw">let</span> (a, s') <span class="fu">=</span> runState m s
                          <span class="kw">in</span> runState (k a) s'</code></pre>
<p>And to compose state carrying functions (using <code>&gt;&gt;=</code>),
 we want to take in a piece of state, evaluate the first function with 
that state, and store the resulting state and value. Then we pass those 
as parameters to the second function. This results in the program state 
getting threaded in and out of both functions.</p>
<p>Let's solidify this a bit by applying it to our AST transformer idea:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- create a type for the state we want to pass around</span>
<span class="kw">type</span> <span class="dt">SexprState</span> <span class="fu">=</span> <span class="dt">State</span> <span class="dt">Int</span>

<span class="co">-- let's wrap an Sexpr in the State monad</span>
<span class="ot">sexprWithState ::</span> <span class="dt">SexprState</span> <span class="dt">Sexpr</span>
sexprWithState <span class="fu">=</span> return <span class="st">"(foo bar)"</span>

<span class="co">-- Now let's run it with the initially state of 0</span>
ghci<span class="fu">&gt;</span> runState sexprWithState <span class="dv">0</span>
(<span class="st">"(foo bar)"</span>, <span class="dv">0</span>)

<span class="co">-- wrap Sexpr in parenthesis</span>
<span class="ot">wrapSexpr ::</span> <span class="dt">Sexpr</span> <span class="ot">-&gt;</span> <span class="dt">SexprState</span> <span class="dt">Sexpr</span>
wrapSexpr exp <span class="fu">=</span> return <span class="fu">$</span> <span class="st">"("</span> <span class="fu">++</span> exp <span class="fu">++</span> <span class="st">")"</span>

<span class="co">-- wrap Sexpr in qux</span>
<span class="ot">addQux ::</span> <span class="dt">Sexpr</span> <span class="ot">-&gt;</span> <span class="dt">SexprState</span> <span class="dt">Sexpr</span>
addQux exp <span class="fu">=</span> return <span class="fu">$</span> <span class="st">"(qux "</span> <span class="fu">++</span> exp <span class="fu">++</span> <span class="st">")"</span>

ghci<span class="fu">&gt;</span> runState (sexprWithState
           <span class="fu">&gt;&gt;=</span> (\exp <span class="ot">-&gt;</span> wrapSexpr exp
           <span class="fu">&gt;&gt;=</span> (\exp2 <span class="ot">-&gt;</span> addQux exp2))) <span class="dv">0</span>
(<span class="st">"(qux ((foo bar)))"</span>,<span class="dv">0</span>)</code></pre>
<p>So now we are doing all of our S-Expression manipulation while 
passing around state in the background. But wait, we aren't doing 
anything with the state!</p>
<p><strong>Accessing and Modifying State</strong></p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="ot">get ::</span> <span class="dt">State</span> s s
get <span class="fu">=</span> <span class="dt">State</span> <span class="fu">$</span> \s <span class="ot">-&gt;</span> (s, s)

<span class="ot">put ::</span> s <span class="ot">-&gt;</span> <span class="dt">State</span> s ()
put s <span class="fu">=</span> <span class="dt">State</span> <span class="fu">$</span> \_ <span class="ot">-&gt;</span> ((), s)

<span class="co">-- example of getting and modifying state</span>
ghci<span class="fu">&gt;</span> runState (sexprWithState
           <span class="fu">&gt;&gt;=</span> wrapSexpr
           <span class="fu">&gt;&gt;=</span> (\exp' <span class="ot">-&gt;</span> get
           <span class="fu">&gt;&gt;=</span> (\counter <span class="ot">-&gt;</span> (put (counter<span class="fu">+</span><span class="dv">1</span>))
           <span class="fu">&gt;&gt;</span> (return exp')
           <span class="fu">&gt;&gt;=</span> addQux))) <span class="dv">0</span>
(<span class="st">"(qux ((foo bar)))"</span>,<span class="dv">1</span>)</code></pre>
<p><strong>transformStmt revisited</strong></p>
<p>With our new found friend the State monad, we can rewrite the original <code>transformStmt</code> function in a monadic style. This enables us to abstract away all the explicit threading of state.</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="ot">transformStmt' ::</span> <span class="dt">Sexpr</span> <span class="ot">-&gt;</span> <span class="dt">SexprState</span> <span class="dt">Sexpr</span>
transformStmt' expr <span class="fu">=</span>
  <span class="co">-- grab the current program state</span>
  get
  <span class="co">-- increment the counter by 1 and store it</span>
  <span class="fu">&gt;&gt;=</span> (\counter <span class="ot">-&gt;</span> (put (counter<span class="fu">+</span><span class="dv">1</span>))
  <span class="co">-- do the sexpr transformation</span>
  <span class="fu">&gt;&gt;</span> (return <span class="fu">$</span> <span class="st">"(define tmpVar"</span> <span class="fu">++</span> (show counter) <span class="fu">++</span> <span class="st">" "</span> <span class="fu">++</span> expr <span class="fu">++</span> <span class="st">")"</span>))

<span class="co">-- And again using do block sugar</span>
<span class="ot">transformStmtDo ::</span> <span class="dt">Sexpr</span> <span class="ot">-&gt;</span> <span class="dt">SexprState</span> <span class="dt">Sexpr</span>
transformStmtDo expr <span class="fu">=</span> <span class="kw">do</span>
  counter <span class="ot">&lt;-</span> get
  put (counter<span class="fu">+</span><span class="dv">1</span>)
  return <span class="fu">$</span> <span class="st">"(define tmpVar"</span> <span class="fu">++</span> (show counter) <span class="fu">++</span> <span class="st">" "</span> <span class="fu">++</span> expr <span class="fu">++</span> <span class="st">")"</span></code></pre>
<h2 id="list-is-a-monad">List is a Monad</h2>
<p>Haskell is known for making easy things really hard. In fact, I was 
never really able iterate over a doubly nested list until I realized 
that lists are monads in Haskell. Specifically, lists are used in a 
monadic style to model nondeterminism, most commonly in Haskell's crazy <a href="http://www.haskell.org/haskellwiki/List_comprehension">list comprehension</a> sugar.</p>
<p>As before, let's try and define <code>return</code> and <code>&gt;&gt;=</code> for the <code>[]</code> type constructor. In the <code>Monad</code> typeclass, <code>return</code> takes type <code>a</code> and wraps it in a type constructor <code>m</code> to give the type <code>m a</code>. So in the case of list, <code>a</code> will be wrapped in the type constructor <code>[]</code>, resulting in <code>[] a</code>, or more easily read as <code>[a]</code>.</p>
<p>To formulate <code>&gt;&gt;=</code> for lists, let us look at the type signature and see if we can find something that matches.</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell">ghci<span class="fu">&gt;</span> <span class="fu">:</span><span class="kw">type</span> (<span class="fu">&gt;&gt;=</span>)
<span class="ot">(&gt;&gt;=) ::</span> (<span class="dt">Monad</span> m) <span class="ot">=&gt;</span> m a <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> m b) <span class="ot">-&gt;</span> m b

ghci<span class="fu">&gt;</span> <span class="fu">:</span><span class="kw">type</span> map
map<span class="ot"> ::</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [b]

ghci<span class="fu">&gt;</span> <span class="fu">:</span><span class="kw">type</span> flip map
flip<span class="ot"> map ::</span> [a] <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> [b]</code></pre>
<p><code>flip map</code> looks close, but we really want a type of <code>[a] -&gt; (a -&gt; [b]) -&gt; [b]</code>. Hence we can substitute <code>[b]</code> for <code>b</code>, resulting in a type of <code>[a] -&gt; (a -&gt; [b]) -&gt; [[b]]</code>, which can then be massaged using <code>concat :: [[a]] -&gt; [a]</code>.</p>
<p>The resulting formulation is:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="kw">instance</span> <span class="dt">Monad</span> [] <span class="kw">where</span>
    return x <span class="fu">=</span> [x]
    xs <span class="fu">&gt;&gt;=</span> f <span class="fu">=</span> concat (map f xs)

    xs <span class="fu">&gt;&gt;</span> f <span class="fu">=</span> concat (map (\_ <span class="ot">-&gt;</span> f) xs)
    fail _ <span class="fu">=</span> []</code></pre>
<p><strong>What does this give us?</strong></p>
<p>Well, list comprehension:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- monadic powerset</span>
ghci<span class="fu">&gt;</span> powerset <span class="fu">=</span> [<span class="dv">1</span>,<span class="dv">2</span>]
             <span class="fu">&gt;&gt;=</span> (\i <span class="ot">-&gt;</span> [<span class="dv">1</span><span class="fu">..</span><span class="dv">4</span>]
             <span class="fu">&gt;&gt;=</span> (\j <span class="ot">-&gt;</span> [(i, j)]))
[(<span class="dv">1</span>,<span class="dv">1</span>),(<span class="dv">1</span>,<span class="dv">2</span>),(<span class="dv">1</span>,<span class="dv">3</span>),(<span class="dv">1</span>,<span class="dv">4</span>),(<span class="dv">2</span>,<span class="dv">1</span>),(<span class="dv">2</span>,<span class="dv">2</span>),(<span class="dv">2</span>,<span class="dv">3</span>),(<span class="dv">2</span>,<span class="dv">4</span>)]


<span class="co">-- or the same using do sugar</span>
<span class="kw">do</span> i <span class="ot">&lt;-</span> [<span class="dv">1</span>,<span class="dv">2</span>]
   j <span class="ot">&lt;-</span> [<span class="dv">1</span><span class="fu">..</span><span class="dv">4</span>]
   return (i,j)

<span class="co">-- or as list comprehension</span>
[(i, j) <span class="fu">|</span> i <span class="ot">&lt;-</span> [<span class="dv">1</span>,<span class="dv">2</span>], j <span class="ot">&lt;-</span> [<span class="dv">1</span><span class="fu">..</span><span class="dv">4</span>]]</code></pre>
<p>For a more detailed explanation and formulation of the list monad, see <a href="http://book.realworldhaskell.org/read/monads.html#id641620">Chapter 14 of Real World Haskell</a>.</p>
<h2 id="desurgaring-do-blocks">Desurgaring Do Blocks</h2>
<p>Ahhh, the sugary sweetness of <code>do</code> blocks. To the novice Haskeller it makes monads look like magic. We've avoided them thus far, but once the desugaring of <code>do</code> blocks is demystified, using <code>do</code> syntax is much more convenient.</p>
<p>So how are <code>do</code> blocks desugared into monadic operators?</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- this do block notation</span>
<span class="kw">do</span> x <span class="ot">&lt;-</span> foo
   bar
<span class="co">-- desugars into:</span>
foo <span class="fu">&gt;&gt;=</span> (\x <span class="ot">-&gt;</span> bar)

<span class="co">-- successive actions</span>
<span class="kw">do</span> act1
   act2
   <span class="fu">...</span>
<span class="co">-- desugar into:</span>
 act1 <span class="fu">&gt;&gt;</span>
 act2 <span class="fu">&gt;&gt;</span>
 <span class="fu">...</span>

<span class="co">-- this do block notation</span>
<span class="kw">do</span> <span class="kw">let</span> x <span class="fu">=</span> expr
       x1 <span class="fu">...</span>
   act1
   act2
   <span class="fu">...</span>
<span class="co">-- desugars into:</span>
<span class="kw">let</span> x <span class="fu">=</span> expr
    x1 <span class="fu">...</span>
<span class="kw">in</span> <span class="kw">do</span> act1
      act2
      <span class="fu">...</span></code></pre>
<h2 id="parametric-guarantees">Parametric Guarantees</h2>
<p>One thing to note is that only types of the kind <code>* -&gt; *</code> (type constructors of arity 1) can be made instances of the <code>Monad</code> typeclass. This means that the values monads wrap are <a href="https://cubeoflambda.wordpress.com/2011/11/16/parametricity/">parametrically polymorphic</a>, ensuring that monadic functions act uniformily on them. That is, <code>&gt;&gt;=</code> and <code>return</code>
 cannot directly manipulate the values they wrap because Haskell doesn't
 support type introspection. This doesn't mean that these functions 
cannot however manipulate other non-polymorphic values present in the 
type constructor. For instance this we can construct a monad that counts
 the number of times <code>&gt;&gt;=</code> is called:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">BindCounter</span> a <span class="fu">=</span> <span class="dt">BindCounter</span> <span class="dt">Int</span> a

<span class="kw">instance</span> <span class="dt">Monad</span> <span class="dt">BindCounter</span> <span class="kw">where</span>
  <span class="co">-- inject</span>
  return a <span class="fu">=</span> <span class="dt">BindCounter</span> <span class="dv">0</span> a
  <span class="co">-- bind</span>
  (<span class="dt">BindCounter</span> x y) <span class="fu">&gt;&gt;=</span> k <span class="fu">=</span> <span class="dt">BindCounter</span> (x<span class="fu">+</span><span class="dv">1</span>) y'
                              <span class="kw">where</span> <span class="dt">BindCounter</span> _ y' <span class="fu">=</span> (k y)

<span class="ot">fooRun ::</span> <span class="dt">IO</span> ()
fooRun <span class="fu">=</span> <span class="kw">do</span>
      <span class="co">-- inject "foo" into the BindCounter Monad</span>
  <span class="kw">let</span> x <span class="fu">=</span> return <span class="st">"foo"</span><span class="ot"> ::</span> <span class="dt">BindCounter</span> <span class="dt">String</span>
      <span class="co">-- do some monadic string appends</span>
      <span class="dt">BindCounter</span> count val <span class="fu">=</span> x
        <span class="fu">&gt;&gt;=</span> (\y <span class="ot">-&gt;</span> <span class="dt">BindCounter</span> <span class="dv">0</span> (y <span class="fu">++</span> <span class="st">" bar"</span>))
        <span class="fu">&gt;&gt;=</span> (\y <span class="ot">-&gt;</span> <span class="dt">BindCounter</span> <span class="dv">0</span> (y <span class="fu">++</span> <span class="st">" baz"</span>))
        <span class="fu">&gt;&gt;=</span> (\y <span class="ot">-&gt;</span> <span class="dt">BindCounter</span> <span class="dv">0</span> (y <span class="fu">++</span> <span class="st">" qux"</span>))
  putStrLn <span class="fu">$</span> <span class="st">"bind count: "</span> <span class="fu">++</span> (show count) <span class="fu">++</span> <span class="st">", resulting value: "</span> <span class="fu">++</span> val

ghci<span class="fu">&gt;</span> fooRun
bind count<span class="fu">:</span> <span class="dv">3</span>, resulting value<span class="fu">:</span> foo bar baz qux</code></pre>
<h2 id="monadic-laws">Monadic Laws</h2>
<p>Monads should also follow 3 laws: right identity, left identity and 
associativity. Haskell doesn't enforce these laws, but if your monads 
don't follow them, people will probably get confused. In addition, these
 laws are required in order for a monad to form a mathematical category,
 which is where the name monad came from <a href="http://www.haskell.org/haskellwiki/Monad_laws">[ref]</a>.</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="co">-- Right Identity:</span>
<span class="co">--   no need to wrap an already wrapped value</span>
m <span class="fu">&gt;&gt;=</span> return <span class="fu">===</span> m

<span class="co">-- do-notation version:</span>
<span class="kw">do</span> { x' <span class="ot">&lt;-</span> return x            <span class="kw">do</span> { f x
     f x' }            <span class="fu">===</span>        }

<span class="co">-- Left Identity:</span>
<span class="co">--   no need to wrap and unwrap a pure value</span>
return x <span class="fu">&gt;&gt;=</span> f <span class="fu">===</span> f x

<span class="co">-- do-notation version:</span>
<span class="kw">do</span> { x <span class="ot">&lt;-</span> m            <span class="fu">===</span>     <span class="kw">do</span> { m
     return x }                   }

<span class="co">-- associativity:</span>
<span class="co">--   preservation of ordering</span>
(m <span class="fu">&gt;&gt;=</span> f) <span class="fu">&gt;&gt;=</span> g  <span class="fu">===</span>  m <span class="fu">&gt;&gt;=</span> (\x <span class="ot">-&gt;</span> f x <span class="fu">&gt;&gt;=</span> g)

<span class="co">-- do-notation version:</span>
<span class="kw">do</span> { y <span class="ot">&lt;-</span> <span class="kw">do</span> { x <span class="ot">&lt;-</span> m          <span class="kw">do</span> { x <span class="ot">&lt;-</span> m
               f x                  <span class="kw">do</span> { y <span class="ot">&lt;-</span> f x
             }         <span class="fu">===</span>               g y
     g y                               }
   }                              }

                               <span class="kw">do</span> { x <span class="ot">&lt;-</span> m
                                    y <span class="ot">&lt;-</span> f x
                       <span class="fu">===</span>          g y
                                  }</code></pre>
<p>We can also define these laws using the monad composition operator defined in Control.Monad <a href="http://www.haskell.org/haskellwiki/Monad_laws">[ref]</a>:</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="ot">(&gt;=&gt;) ::</span> <span class="dt">Monad</span> m <span class="ot">=&gt;</span> (a <span class="ot">-&gt;</span> m b) <span class="ot">-&gt;</span> (b <span class="ot">-&gt;</span> m c) <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> m c
m <span class="fu">&gt;=&gt;</span> n <span class="fu">=</span> \x <span class="ot">-&gt;</span> <span class="kw">do</span> { y <span class="ot">&lt;-</span> m x; n y }

<span class="co">-- Left identity</span>
return <span class="fu">&gt;=&gt;</span> g <span class="fu">===</span> g

<span class="co">-- Right identity</span>
f <span class="fu">&gt;=&gt;</span> return <span class="fu">===</span> f

<span class="co">-- Associativity</span>
(f <span class="fu">&gt;=&gt;</span> g) <span class="fu">&gt;=&gt;</span> h <span class="fu">===</span> f <span class="fu">&gt;=&gt;</span> (g <span class="fu">&gt;=&gt;</span> h)</code></pre>
<h2 id="resources">Resources</h2>
<ul>
<li>Corresponding Monad Slides <a href="http://www.ccs.neu.edu/files/monad-slides.pdf">PDF</a> &amp; <a href="http://www.ccs.neu.edu/files/monad-slides.tex">Beamer</a></li>
<li>Example Code: [<a href="http://www.ccs.neu.edu/code/monads/MaybeMonad.hs">maybe monad</a>, <a href="http://www.ccs.neu.edu/code/monads/SexprState.hs">state monad</a>]</li>
</ul>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://en.wikibooks.org/wiki/Haskell/Understanding_monads">Haskell WikiBook: Understanding Monads</a></li>
<li>Chapters <a href="http://book.realworldhaskell.org/read/monads.html">14</a>, <a href="http://book.realworldhaskell.org/read/programming-with-monads.html">15</a>, and <a href="http://book.realworldhaskell.org/read/monad-transformers.html">18</a> of <a href="http://book.realworldhaskell.org/">Real World Haskell</a> by Bryan O'Sullivan, Don Stewart, and John Goerzen</li>
</ul></div></div><div style="clear: both;"></div></div>
    </div>
  </body>
  <script type="application/javascript" src="chrome://browser/content/feeds/subscribe.js"></script>
</html>